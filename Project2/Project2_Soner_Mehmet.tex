% LaTeX template for DLD Lab - runs with MikTeX and other platforms

\documentclass{article}
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{setspace}
\usepackage{verbatim}

\usepackage[dvipsnames]{xcolor}
\usepackage{matlab-prettifier}

\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}[section]
\newtheorem{dfn}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{rem}[thm]{Remark}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{asm}[thm]{Assumption}
\newtheorem{example}[thm]{Example}

\newenvironment{proof}{\noindent {\bf Proof.\/}}{$\qed$\vskip 0.1in}
\def\qed{ \hfill \vrule width.2cm height.2cm depth0cm\smallskip}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{pythonhighlight}

\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}




\numberwithin{equation}{section}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
%greeks
\newcommand{\te}{{\theta}}
\newcommand{\Te}{{\Theta}}
\newcommand{\vt}{{\vartheta}}
\newcommand{\Om}{{\Omega}}
\newcommand{\om}{{\omega}}
\newcommand{\ups}{{\upsilon}}
\newcommand{\ve}{{\varepsilon}}
\newcommand{\del}{{\delta}}
\newcommand{\Del}{{\Delta}}
\newcommand{\gam}{{\gamma}}
\newcommand{\Gam}{{\Gamma}}
\newcommand{\vf}{{\varphi}}
\newcommand{\Sig}{{\Sigma}}
\newcommand{\sig}{{\sigma}}
\newcommand{\al}{{\alpha}}
\newcommand{\be}{{\beta}}
\newcommand{\ka}{{\kappa}}
\newcommand{\la}{{\lambda}}
\newcommand{\La}{{\Lambda}}


\def \D{\mathbb{D}}
\def \E{\mathbb{E}}
\def \F{\mathbb{F}}
\def \H{\mathbb{H}}
\def \L{\mathbb{L}}
\def \M{\mathbb{M}}
\def \N{\mathbb{N}}
\def \P{\mathbb{P}}
\def \Q{\mathbb{Q}}
\def \R{\mathbb{R}}
\def \Z{\mathbb{Z}}
\def \Sb{\mathbb {S}}

\def \om{\omega}
\def \Om{\Omega}
\def \ep{\epsilon}

\def\reff#1{{\rm(\ref{#1})}}

\usepackage{times}	   % uncomment to use Times-Roman fonts
%\usepackage{mathpazo}     % uncomment to use Palatino fonts
\usepackage{amsmath}	   % enable amsmath features
\usepackage{graphicx}      % enable inclusion of eps graphs
\usepackage{cite}          % bibliographical citations
\usepackage{url}           % typesetting URL's
\usepackage{color}

% ---------------------------------------------------------------

\setlength{\textwidth}{5.75in}            
\setlength{\oddsidemargin}{0.375in}   % textwidth + 2*oddsidemargin = 6.5
\setlength{\evensidemargin}{0.375in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}

\def\ce{\begin{center}}            
\def\cend{\end{center}}

\def\red{\color{red}}
\def\blue{\color{blue}}
\def\black{\color{black}}

\begin{document}

\ce
\red\Large
RUTGERS UNIVERSITY \\[0.05in]
School of Engineering \\[0.05in]
Department of Electrical \& Computer Engineering \\[0.2in]
\blue ECE 472 -- Robotics \& Computer Vision-- Fall 2022
\cend

\vspace{1in}

\huge \blue 

\begin{center}
Project 2 - Reinforcement Learning
\end{center}

\vspace{1in}

\Large

Name (last, first) : \ Mehmet Ali Soner 

\vspace{0.3in}

netID : \ mas996

\vspace{0.3in}

RUID:  196000499

\vspace{0.3in}

Date: \today




\vspace{1in}

\color{black} \normalsize


\newpage




\section{Problem 1}
Cart Pole code assignment, code + explanation




\textbf{Important:} For google colab and displaying videos, use code below:

\begin{python}
# install dependencies needed for recording videos
!apt-get install -y xvfb x11-utils
!pip install pyvirtualdisplay==0.2.*


from pyvirtualdisplay import Display
display = Display(visible=False, size=(1400, 900))
_ = display.start()

from gym.wrappers.monitoring.video_recorder import VideoRecorder
before_training = "before_training.mp4"

video = VideoRecorder(env, before_training)
# returns an initial observation
env.reset()
for i in range(200):
  env.render()
  video.capture_frame()
  # env.action_space.sample() produces either 0 (left) or 1 (right).
  observation, reward, done, info = env.step(env.action_space.sample())
  # Not printing this time
  #print("step", i, observation, reward, done, info)

video.close()
env.close()


from base64 import b64encode
def render_mp4(videopath: str) -> str:
  """
  Gets a string containing a b4-encoded version of the MP4 video
  at the specified path.
  """
  mp4 = open(videopath, 'rb').read()
  base64_encoded_mp4 = b64encode(mp4).decode()
  return f'<video width=400 controls><source src="data:video/mp4;' \
         f'base64,{base64_encoded_mp4}" type="video/mp4"></video>'
         
         
from IPython.display import HTML
html = render_mp4(before_training)
HTML(html)

\end{python}




\section{Problem 2}
Explain DQN algorithm in paragraphs, include definitions of state, action,environment, reward.

Notes: Agents and environment: agent(s) interacts with environment, which can be a simulation, such as the cart pole example. Each step, the agent observes the environment (state of environment) and takes action and receives a reward based on that. Agents learn from repeated trials; these are called episodes. RL framework trains a policy for the agent to follow. Policy shows which actions to take one after the other in order to maximize reward. \\

In RL, want to train the agent to make better decision or act better with each episode. Policy == neural network. 

\begin{python}

# returns an initial observation
env.reset()

for i in range(20):

  # env.action_space.sample() produces either 0 (left) or 1 (right).
  observation, reward, done, info = env.step(env.action_space.sample())

  print("step", i, observation, reward, done, info)

env.close()
\end{python}

step: how many times through environment. observation = [x cart, y cart, pole angle, pole ang. velocity]. reward = 1 for each step. done = when episode is done (maybe pole tipped over too much)





\section{Problem 3}
Performance metrics and plots








\section{Problem 4}
Three other problems for RL; state, action, environment and reward for each


\begin{comment}
\begin{figure}
	\centering
	\hspace*{-3.0cm}
	\includegraphics[scale=0.0001]{Q4.2M.png}
	\\	
	\textbf{Fig.7:} Comparator for 3-bit signed integers, $a=-2=[1,1,0]$
	\\
	\label{fig:Fig.7}
\end{figure}
\end{comment}













\end{document}