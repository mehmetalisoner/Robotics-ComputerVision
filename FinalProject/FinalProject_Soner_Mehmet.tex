% LaTeX template for DLD Lab - runs with MikTeX and other platforms

\documentclass{article}
\usepackage{mathptmx}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{setspace}
\usepackage{verbatim}

\usepackage[dvipsnames]{xcolor}
\usepackage{matlab-prettifier}
\usepackage{float}
\usepackage[export]{adjustbox}



\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}[section]
\newtheorem{dfn}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{rem}[thm]{Remark}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{asm}[thm]{Assumption}
\newtheorem{example}[thm]{Example}

\newenvironment{proof}{\noindent {\bf Proof.\/}}{$\qed$\vskip 0.1in}
\def\qed{ \hfill \vrule width.2cm height.2cm depth0cm\smallskip}

\usepackage{xcolor}
\usepackage{listings}
\usepackage{pythonhighlight}

\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays


% For minted -> Python
\usepackage{tcolorbox}
\tcbuselibrary{minted,breakable,xparse,skins}
\definecolor{bg}{gray}{0.95}
\DeclareTCBListing{mintedbox}{O{}m!O{}}{%
  breakable=true,
  listing engine=minted,
  listing only,
  minted language=#2,
  minted style=default,
  minted options={%
    linenos,
    gobble=0,
    breaklines=true,
    breakafter=,,
    fontsize=\small,
    numbersep=8pt,
    #1},
  boxsep=0pt,
  left skip=0pt,
  right skip=0pt,
  left=25pt,
  right=0pt,
  top=3pt,
  bottom=3pt,
  arc=5pt,
  leftrule=0pt,
  rightrule=0pt,
  bottomrule=2pt,
  toprule=2pt,
  colback=bg,
  colframe=orange!70,
  enhanced,
  overlay={%
    \begin{tcbclipinterior}
    \fill[orange!20!white] (frame.south west) rectangle ([xshift=20pt]frame.north west);
    \end{tcbclipinterior}},
  #3}



\numberwithin{equation}{section}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
%greeks
\newcommand{\te}{{\theta}}
\newcommand{\Te}{{\Theta}}
\newcommand{\vt}{{\vartheta}}
\newcommand{\Om}{{\Omega}}
\newcommand{\om}{{\omega}}
\newcommand{\ups}{{\upsilon}}
\newcommand{\ve}{{\varepsilon}}
\newcommand{\del}{{\delta}}
\newcommand{\Del}{{\Delta}}
\newcommand{\gam}{{\gamma}}
\newcommand{\Gam}{{\Gamma}}
\newcommand{\vf}{{\varphi}}
\newcommand{\Sig}{{\Sigma}}
\newcommand{\sig}{{\sigma}}
\newcommand{\al}{{\alpha}}
\newcommand{\be}{{\beta}}
\newcommand{\ka}{{\kappa}}
\newcommand{\la}{{\lambda}}
\newcommand{\La}{{\Lambda}}


\def \D{\mathbb{D}}
\def \E{\mathbb{E}}
\def \F{\mathbb{F}}
\def \H{\mathbb{H}}
\def \L{\mathbb{L}}
\def \M{\mathbb{M}}
\def \N{\mathbb{N}}
\def \P{\mathbb{P}}
\def \Q{\mathbb{Q}}
\def \R{\mathbb{R}}
\def \Z{\mathbb{Z}}
\def \Sb{\mathbb {S}}

\def \om{\omega}
\def \Om{\Omega}
\def \ep{\epsilon}

\def\reff#1{{\rm(\ref{#1})}}

\usepackage{times}	   % uncomment to use Times-Roman fonts
%\usepackage{mathpazo}     % uncomment to use Palatino fonts
\usepackage{amsmath}	   % enable amsmath features
\usepackage{graphicx}      % enable inclusion of eps graphs
\usepackage{cite}          % bibliographical citations
\usepackage{url}           % typesetting URL's
\usepackage{color}

% ---------------------------------------------------------------

\setlength{\textwidth}{5.75in}            
\setlength{\oddsidemargin}{0.375in}   % textwidth + 2*oddsidemargin = 6.5
\setlength{\evensidemargin}{0.375in}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}

\def\ce{\begin{center}}            
\def\cend{\end{center}}

\def\red{\color{red}}
\def\blue{\color{blue}}
\def\black{\color{black}}

\begin{document}

\ce
\red\Large
RUTGERS UNIVERSITY \\[0.05in]
School of Engineering \\[0.05in]
Department of Electrical \& Computer Engineering \\[0.2in]
\blue ECE 472 -- Robotics \& Computer Vision-- Fall 2022
\cend

\vspace{1in}

\huge \blue 

\begin{center}
Final Project - Reinforcement Learning: Super Mario Bros.
\end{center}

\vspace{1in}

\Large

Name (last, first) : \ Mehmet Ali Soner 

\vspace{0.3in}

netID : \ mas996

\vspace{0.3in}

RUID:  196000499

\vspace{0.3in}

Date: \today




\vspace{1in}

\color{black} \normalsize


\newpage




\section*{Introduction}
For the final project of this course, I wanted to create and train my own agent for a video game of my choice. I got heavily interested in reinforcement learning after completing the CartPole example on the official PyTorch page. The fact that you can train an agent to do certain tasks seemed so absurd and out of this world. My ultimate goal is to create my own implementation of the first example of reinforcement learning that I witnessed: an autonomous vehicle in a 3-D open-world game with heavy traffic. At that time I didn't know that this was reinforcement learning but looking back now, things are a bit clearer. Although this is my ultimate goal, I wanted to get familiar with training agents in much simpler environments. Thus, I chose the to train one of the most popular Nintendo characters: Mario. The specific game which I wanted to train my agent in was the original 1983 Super Mario Bros, which came out on the NES. I have played it on newer versions of the NES and I absolutely enjoyed it. I wanted to train my agent in the first level of the game, which isn't hard to complete as a human. So, I was curious on how an agent would perform in a rather simple but still more challenging than CartPole, since there is an increased amount of factors that can influence the behavior of the agent. 



\section*{Methods}

\subsection*{Environments}
In this project, I took multiple approaches ranging from utilizing various wrappers to fine tuning. I also tried different models and policies with increasing total time steps. First, let me explain the details of the specific environment that I found for this use case. The environment is called gym-super-mario-bros and can be installed with a simple pip command. 
\begin{minted}
!pip install gym-super-mario-bros
\end{minted}

The documentation for this environment can be found under: \href{https://pypi.org/project/gym-super-mario-bros/}. In the documentation, there are various versions of this particular environment:
\begin{enumerate}
\item standard ROM
\item downsampled ROM
\item pixel ROM
\item rectangle ROM
\end{enumerate}

The first environment is the classic game without any changes, the second one only contains the most important pieces of information (e.g.: clouds and the sky are not visible, they are black), the third one is heavily pixelated and the last environment renders objects in the shape of rectangles. I wanted to go with the standard game since that would be a lot more impressive and interesting to demo. The action space here can be chosen by the user. There is a selection of three movement types, namely:

\begin{enumarte}
\item RIGHT_ONLY
\item SIMPLE_MOVEMENT
\item COMPLEX_MOVEMENT
\end{enumaree}

The first type of actions is self-explanatory, there are only movements that contain a 'right' movement. The second movement type differs slightly from the first one. It contains a 'left' movement and a straight jump up, 'A'. The last one contains almost possible direction that you can have on an actual joypad, including crouching, standing up, jumping back and more. This is the code for each movement type on their github:

\begin{mintedbox}{python}

 actions for the simple run right environment
RIGHT_ONLY = [
    ['NOOP'],
    ['right'],
    ['right', 'A'],
    ['right', 'B'],
    ['right', 'A', 'B'],
]


# actions for very simple movement
SIMPLE_MOVEMENT = [
    ['NOOP'],
    ['right'],
    ['right', 'A'],
    ['right', 'B'],
    ['right', 'A', 'B'],
    ['A'],
    ['left'],
]


# actions for more complex movement
COMPLEX_MOVEMENT = [
    ['NOOP'],
    ['right'],
    ['right', 'A'],
    ['right', 'B'],
    ['right', 'A', 'B'],
    ['A'],
    ['left'],
    ['left', 'A'],
    ['left', 'B'],
    ['left', 'A', 'B'],
    ['down'],
    ['up'],
]
\end{mindtedbox}

'NOOP' stands for no operation, so the agent stands still. 'A' stands for jumping, 'B' stands for sprinting and the direction are ['right','up','left','down']. These movement types are imported through the simple command in any Python file:
\begin{mintedbox}{python}
from nes_py.wrappers import JoypadSpace
import gym_super_mario_bros
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT
env = gym_super_mario_bros.make('SuperMarioBros-v0')
env = JoypadSpace(env, SIMPLE_MOVEMENT)
\end{mintedbox}

This is how we would setup the most basic environment to train our agent in. As mentioned before, we install the gym-super-mario-bros package with pip, import that specific package, and wrap the movement type of choice with a JoypadSpace wrapper around our environment. This is sufficient enough to train our agent however in order to improve performance or if we want to train our model for large amount of total time steps, we need add some additional wrappers for that. We want good data/environment in order to create a good policy.


\subsection*{Additional Wrappers}








\begin{itemize}
\item the states: 
\begin{itemize}
\item the cart's x-coordinate
\item cart's velocity
\item the pole's angular velocity
\item the pole's angle
\end{itemize}
\item the environment: the simulation that is simulating cart with the pole that is on a rail
\item actions:
\begin{itemize}
\item move cart to right
\item move cart to left
\end{itemize} 
\item reward:
\begin{itemize}
\item reward = 1, when the pole stays within the specified angle (±12°) and cart's x-coordinate is in range of (-2.4, 2.4)
\item reward = 0, when the above mentioned conditions are not met, then we fail and restart the environment
\end{itemize} 
\end{itemize}  

Essentially we want:
$$
\pi^*(s) = \argmax_a \ Q^*(s,a)
$$
However, we cannot achieve this since we don't know every possible state, action and reward in the world. What we do instead is, we try to approximate the $Q$ function with the help of neural networks. We assume that the $Q$ function is a Bellman equation and can write it like this:

$$
Q^{\pi}(s,a) = r_0 + \gamma Q^{\pi}(s',\pi(s'))
$$
This equation describes that the current $Q$ function is the sum of the current reward $r_0$ and the $Q$ function of the future state $s'$ and action $a' = \pi(s')$ multiplied by a discount rate $\gamma$. This is a very powerful tool since we can describe the $Q$ function recursively.

We can rewrite this equation into what will be inputted into the Huber loss.
For the Huber loss function, we input what is called the temporal difference error, $\delta$:
$$
\delta = Q(s,a) - \left(r_0 + \gamma \max_a Q(s',\pi(s'))\right)
$$
The first term of the subtraction is the prediction, the values that we get when we take an action $a$ in the state $s$. We get this value when we are inputting the state\_batch into the policy\_net to get the state\_action\_values. The second term of the subtraction is the target value that we obtain by passing through the target\_net with non\_final\_next\_states. The results of that pass through are the next\_state\_values. We then multiply the next\_state\_values by $\gamma$ and add the reward batch to it. $s'$ would be the future state, $a'$ the future action.  We want to minimize this error since we want to have our prediction and target values as close as possible to each other. We do that by inputting that into a Huber loss function. 










\section*{Results}
For performance metrics, we can observe the duration of each episode or the reward. This gives us basic understanding of how well the model performs. For every action where the pole is still upright, we add +1 to the reward variable we have or we add up every action that didn't result into a final state and include the terminating state as well. We do this with a for loop and the iteration variable in the loop condition:


\begin{mintedbox}{python}
for t in count():
	if isRandom  == True:
	  action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)
	else:
	  action = select_action(state,policy_net)
	observation, reward, terminated, truncated, _ = env.step(action.item())
	reward = torch.tensor([reward], device=device)
	done = terminated or truncated

	...
	
	if done:
		episode_durations.append(t + 1)
		break
\end{mintedbox}

This is how the loop for each episode looks like. When we have a terminal state, then we go into the if statement on line 12 and add how many steps we have taken, which equals to the duration of one episode. We then plot these duration with a helper plotting function. Here are the result of three scenarios, namely:
\begin{enumerate}
\item A random agent with num\_episodes = 600
\item A trained agent with num\_episodes = 100
\item A trained agent with num\_episodes = 600
\end{enumerate}


Here are the results in that order:


The results are as expected. The worst performance out of the three is with the random agent, the second worst is the agent with the small training time (or less number of episodes) and the best performance comes from the trained agent with the highest training time. In Fig.1, we can observe that the episode duration on average stays constant and has a value of 25-30. This makes sense since the agent doesn't update it's policy net efficiently because it is selecting random action. There are some outliers that happen (peak at 79 and 70) because of the randomness of the action selection module. The module might take the all of the right actions in one episode. However, the probability of that happening is low because it has to randomly select all of the actions successively and it is reflected in this plot. 

The second figure, Fig.2, shows more promising results. The performance in the first 70 episodes is in fact worse the the first 70 episodes with a random agent but there is sharp jump in performance around episode 80. Here, we can see that the performance significantly increases and stays at this level. We can also conclude that training an agent for 100 episodes isn't quite enough since it doesn't pass the "solved" threshold of 200 in this environment. The results seem promising however and we can predict that the duration of each episode is going to increase with more training. This agent outperforms the random agent even though it had 1/6 of episodes to train in.

The third figure, Fig.3, proves our prediction we had with Fig.2. Again, around 100 episodes there is a sharp peak that seems to be constant until episode = 200. From there, there is a linear increase in performance until episode = 450 from where the performance increase improves even further. The performance increase is a lot steeper. It is also around episode = 450, where the module passes the "solved" threshold of 200. After some time, the module cannot improve any further and it stagnates at around episode = 550. This is all expected and seems ordinary. What is surprising is that both train models had much worse performance in the first 60-80 episodes than the random agent. The performance in the first hundred episode also seem to decrease when we increase the number of episodes we want the model to simulate. This doesn't matter since we can arrive a higher episode duration or overall reward in the long run.


\section*{Conclusion}
Here are three other problems that can be solved with RL on open-AI gym:

\begin{enumerate}
\item Mountain Car (Classic Control):
\begin{itemize}
\item State: In this environment, there are only two states. The position of the car along the x-axis and the velocity of the car. The x-coordinate and the velocity of the car can from $\left(-\infty, \infty\right)$.
\item Action: There are three actions in this environment. Accelerate to the right, accelerate to the left or don't accelerate at all.  
\item Environment: A car is at the bottom of a "sinusoidal valley" and has to get out of that valley by reaching the flag at the top of the hill. The starting position of the car is a random value in the range of [-0.6,-0.4]. The episode ends when the car's x-coordinate is greater than or equal to 0.5, which means it has reached the flag or when the episode length is over 200.
\item Reward: In this environment, the reward is -1 for each timestep where the car hasn't reached the flag on top of the hill. So, the car has to reach the flag as quickly as possible.  
\end{itemize}



\item Car Racing (Box2D): 
\begin{itemize}
\item State: In this environment, the observation space consists of 96x96 pixels, which determine the track and the position of the racing car.
\item Action: There are three actions if the environment is continuous and 5 if discrete. In the continuous scenario we have: steering (-1 full left, +1 full right), gas and break. In the discrete scenario, we have: do nothing, turn right, turn left, gas, brake.
\item Environment: A racing car is on a racing track and is trying to complete it. It starts at the starting point of the track, in the middle of center of the road. The episode ends when all tiles have been visited. The racing car can go off the track but it will get rewarded with -100 and "die" per the Gym documentation.
\item Reward: In this environment, the reward is -0.1 for every frame that the car took to get to the episode's end and +1000/N, where N is the amount of tiles that the car visited.
\end{itemize}









\item : Frozen Lake (Toy Text):
\begin{itemize}
\item State: In this environment, the observation space consists of one numeric value, which indicates the position of the agent on the lake. It can be calculated with $agent{position} = current\_row * nrows + current\_col$.
\item Action: There are four actions that the agent can take in this environment. Go up, left, down and right.
\item Environment: An agent has to reach a goal $G$ on a frozen lake with holes $H$ from the starting point $S$. The main objective is to reach the goal \textbf{without} falling into one of the holes. The agent's movement may not be clear since it will not move into the direction of the goal all the time. This is simply because of the slippery nature of the frozen lake.
\item Reward: In this environment, the only time you get a +1 is when you reach the goal. All other actions resulting into reaching a hole or any frozen tile, "pay out" a 0 as reward.
\end{itemize}


\end{enumerate}





\begin{comment}

\begin{figure}[H]
	\centering
	
	\includegraphics[width=\linewidth,cframe=blue 2.5pt 2.5pt]{agent600.png}
	\\	
	\vspace{0.1in}
	\textbf{Fig.3:} A trained agent with num\_episodes = 600
	\\
	\label{fig:Fig.3}
\end{figure}
\end{comment}













\end{document}